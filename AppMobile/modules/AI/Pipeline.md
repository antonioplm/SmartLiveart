# ðŸ“˜ **Documento Tecnico â€“ Pipeline Audio Conversazionale Mobile (STT â†’ AI â†’ TTS â†’ Lipsync â†’ Avatar)**  
**Versione:** 1.0  
**Ambito:** App mobile Unity (Android + iOS) per dialogo vocale con avatar

---

# 1. Introduzione
Lâ€™obiettivo del sistema Ã¨ permettere un dialogo naturale tra un utente e un avatar 3D, tramite:

- input vocale dellâ€™utente  
- riconoscimento vocale (STT)  
- elaborazione AI su backend  
- generazione vocale (TTS)  
- riproduzione audio  
- lipsync in tempo reale sullâ€™avatar  

La discussione ha analizzato:

- test del TTS  
- pipeline completa  
- architetture possibili  
- limiti e capacitÃ  dei device mobile  
- scelta ottimale per un pubblico eterogeneo (studenti, turisti)  
- STT locale vs cloud  
- TTS locale vs cloud  
- streaming vs pipeline sequenziale  

---

# 2. Test del TTS: obiettivo e pipeline
Il punto di partenza Ã¨ stato il test della pipeline:

```
Testo â†’ TTS â†’ AudioClip â†’ Lipsync â†’ Avatar
```

Il test serve a verificare:

- che il TTS generi audio valido  
- che Unity carichi correttamente lâ€™audio  
- che il lipsync riceva campioni reali  
- che lâ€™avatar si muova in sync  

Ãˆ stato definito uno script di test per:

- caricare un file WAV/MP3  
- riprodurlo tramite AudioSource  
- collegarlo al sistema di lipsync  

---

# 3. Pipeline completa di dialogo vocale
La pipeline conversazionale completa Ã¨:

```
Voce utente â†’ STT â†’ Testo â†’ Backend AI â†’ Testo â†’ TTS â†’ Audio â†’ Lipsync â†’ Avatar
```

Il backend AI risponde tramite **JSON**, e lâ€™app deve:

1. estrarre il testo  
2. inviarlo al TTS  
3. riprodurre lâ€™audio  
4. pilotare il lipsync  

---

# 4. Esistono pipeline piÃ¹ veloci?
SÃ¬. La pipeline classica Ã¨ funzionale ma non ottimale in termini di latenza.

Sono state analizzate 4 architetture piÃ¹ veloci:

### 4.1 Streaming completo
```
STT streaming â†’ AI streaming â†’ TTS streaming â†’ lipsync
```
Lâ€™avatar inizia a parlare entro **200â€“400 ms**.

### 4.2 TTS predittivo
Il TTS inizia a generare audio prima che la frase sia completa.

### 4.3 TTS con visemi integrati
Il TTS genera direttamente:

- audio  
- visemi  
- durata fonemi  

Riduce la pipeline a:

```
Testo â†’ TTS (audio + visemi) â†’ Avatar
```

### 4.4 AI vocale endâ€‘toâ€‘end
Un unico modello trasforma voce in voce, senza testo intermedio.

---

# 5. JSON rimane compatibile con lo streaming
Ãˆ stato chiarito che:

ðŸ‘‰ **Lo streaming non elimina il JSON.**

Il backend puÃ² continuare a inviare:

- chunk JSON  
- eventi JSON (SSE)  
- pacchetti JSON con audio base64  
- token parziali  

Esempio:

```
{"partial_text": "Certo,"}
{"partial_text": "posso"}
{"partial_text": "aiutarti."}
{"done": true}
```

---

# 6. STT e TTS: lato app o lato backend?
Sono state analizzate le due possibilitÃ :

## 6.1 STT/TTS lato app
Pro:
- latenza bassissima  
- offline  

Contro:
- modelli pesanti  
- consumo CPU  
- qualitÃ  inferiore  
- non uniforme su Android  

## 6.2 STT/TTS lato backend
Pro:
- qualitÃ  alta  
- modelli aggiornabili  
- nessun carico sul device  

Contro:
- latenza di rete  
- dipendenza dalla connessione  

## 6.3 Architettura ibrida (consigliata)
```
STT locale â†’ AI cloud â†’ TTS cloud â†’ lipsync locale
```

Ãˆ la soluzione piÃ¹ veloce e piÃ¹ stabile per mobile.

---

# 7. Considerazioni specifiche per app mobile (Android + iOS)
Lâ€™app Ã¨ sviluppata in Unity per Android e iOS.

### 7.1 STT locale su iOS
- SFSpeechRecognizer  
- molto affidabile  
- offline per molte lingue  
- latenza bassa  

### 7.2 STT locale su Android
La situazione Ã¨ piÃ¹ complessa:

- SpeechRecognizer esiste su quasi tutti i device  
- ma **non garantisce** STT offline  
- alcuni device non hanno modelli offline  
- alcuni device non hanno Google Services  
- alcuni device usano solo STT cloud  
- alcuni device non supportano STT affatto  

Conclusione:

ðŸ‘‰ **STT locale Android non Ã¨ affidabile al 100% su device eterogenei.**

---

# 8. Caso dâ€™uso reale: studenti con smartphone personali
La sperimentazione iniziale coinvolge studenti liceali con smartphone personali.

Questo implica:

- device molto diversi  
- versioni Android diverse  
- alcuni con Google, altri senza  
- alcuni con STT offline, altri no  
- qualitÃ  microfoni variabile  

Per questo:

ðŸ‘‰ **Non si puÃ² basare lâ€™esperienza solo su STT locale Android.**

---

# 9. Architettura STT consigliata per studenti e turisti
La soluzione ottimale Ã¨ una pipeline **ibrida con fallback automatico**.

## 9.1 Pipeline consigliata
```
STT locale (se disponibile)
â†“ fallback automatico
STT cloud
â†“
Backend AI (JSON)
â†“
TTS cloud (streaming)
â†“
AudioClip + Lipsync locale
â†“
Avatar
```

## 9.2 Vantaggi
- compatibilitÃ  totale  
- latenza ottimizzata  
- qualitÃ  vocale uniforme  
- nessun blocco su device problematici  
- esperienza fluida anche con rete debole  

---

# 10. Scelta finale raccomandata
Per unâ€™app mobile Unity destinata a:

- studenti  
- turisti  
- pubblico eterogeneo  

la pipeline migliore Ã¨:

### âœ” STT locale nativo (iOS + Android)  
### âœ” fallback automatico a STT cloud  
### âœ” AI cloud con JSON streaming  
### âœ” TTS cloud streaming  
### âœ” lipsync locale in Unity  

Questa architettura garantisce:

- latenza bassa  
- qualitÃ  alta  
- compatibilitÃ  totale  
- scalabilitÃ   
- esperienza naturale  

---

# 11. Conclusione
La discussione ha portato a definire una pipeline conversazionale **robusta**, **scalabile** e **ottimizzata per mobile**, capace di funzionare su smartphone eterogenei e in contesti reali come scuole e turismo.

La soluzione finale Ã¨ unâ€™architettura **ibrida**, con:

- STT locale quando possibile  
- fallback cloud quando necessario  
- backend AI centralizzato  
- TTS streaming  
- lipsync locale  

Il tutto mantenendo **JSON come formato di scambio**, anche in modalitÃ  streaming.

---
